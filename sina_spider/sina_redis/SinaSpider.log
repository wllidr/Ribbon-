2018-07-25 19:40:34 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: ['sina', 'sina_fans'])
2018-07-25 19:40:34 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.9.0, Python 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:04:45) [MSC v.1900 32 bit (Intel)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-07-25 19:40:34 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': ['sina', 'sina_fans'], 'COMMANDS_MODULE': 'sina.commands', 'DOWNLOAD_DELAY': 2, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_FILE': 'SinaSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'sina.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['sina.spiders']}
2018-07-25 19:40:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-07-25 19:40:34 [twisted] CRITICAL: Unhandled error in Deferred:
2018-07-25 19:40:34 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\spiders.py", line 153, in from_crawler
    obj = super(RedisSpider, self).from_crawler(crawler, *args, **kwargs)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\spiders\sina_fans.py", line 25, in __init__
    super(SinaSpider, self).__init__(*args, **kwargs)  # 当前自定义类
NameError: name 'SinaSpider' is not defined
2018-07-25 19:40:34 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': ['sina', 'sina_fans'], 'COMMANDS_MODULE': 'sina.commands', 'DOWNLOAD_DELAY': 2, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_FILE': 'SinaSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'sina.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['sina.spiders']}
2018-07-25 19:40:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-07-25 19:40:34 [sina] INFO: Reading start URLs from redis key 'sinaspider:start_urls' (batch size: 16, encoding: utf-8
2018-07-25 19:40:40 [twisted] CRITICAL: Unhandled error in Deferred:
2018-07-25 19:40:40 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 80, in crawl
    self.engine = self._create_engine()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 105, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\downloader\__init__.py", line 88, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\misc.py", line 44, in load_object
    mod = import_module(module)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 994, in _gcd_import
  File "<frozen importlib._bootstrap>", line 971, in _find_and_load
  File "<frozen importlib._bootstrap>", line 955, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\middlewares.py", line 10, in <module>
    from sina.util.cookies import cookies
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\cookies.py", line 158, in <module>
    cookie = processCookies(i['user'], i['pwd'])
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\cookies.py", line 137, in processCookies
    browser = webdriver.Chrome()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\selenium\webdriver\chrome\webdriver.py", line 68, in __init__
    self.service.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'chromedriver' executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home

2018-07-25 19:48:23 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: ['sina'])
2018-07-25 19:48:23 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.9.0, Python 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:04:45) [MSC v.1900 32 bit (Intel)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-07-25 19:48:23 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': ['sina'], 'COMMANDS_MODULE': 'sina.commands', 'DOWNLOAD_DELAY': 2, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_FILE': 'SinaSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'sina.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['sina.spiders']}
2018-07-25 19:48:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-07-25 19:48:23 [sina] INFO: Reading start URLs from redis key 'sinaspider:start_urls' (batch size: 16, encoding: utf-8
2018-07-25 19:48:23 [twisted] CRITICAL: Unhandled error in Deferred:
2018-07-25 19:48:23 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 80, in crawl
    self.engine = self._create_engine()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 105, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\downloader\__init__.py", line 88, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\misc.py", line 44, in load_object
    mod = import_module(module)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 994, in _gcd_import
  File "<frozen importlib._bootstrap>", line 971, in _find_and_load
  File "<frozen importlib._bootstrap>", line 955, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\middlewares.py", line 10, in <module>
    from sina.util.cookies import cookies
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\cookies.py", line 158, in <module>
    cookie = processCookies(i['user'], i['pwd'])
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\cookies.py", line 137, in processCookies
    browser = webdriver.Chrome()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\selenium\webdriver\chrome\webdriver.py", line 68, in __init__
    self.service.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'chromedriver' executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home

2018-07-25 19:48:23 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': ['sina'], 'COMMANDS_MODULE': 'sina.commands', 'DOWNLOAD_DELAY': 2, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_FILE': 'SinaSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'sina.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['sina.spiders']}
2018-07-25 19:48:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-07-25 19:48:23 [sina_fans] INFO: Reading start URLs from redis key 'SinaFansSpider:start_urls' (batch size: 16, encoding: utf-8
2018-07-25 19:48:23 [twisted] CRITICAL: Unhandled error in Deferred:
2018-07-25 19:48:23 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 80, in crawl
    self.engine = self._create_engine()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 105, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\downloader\__init__.py", line 88, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\misc.py", line 44, in load_object
    mod = import_module(module)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 994, in _gcd_import
  File "<frozen importlib._bootstrap>", line 971, in _find_and_load
  File "<frozen importlib._bootstrap>", line 955, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\middlewares.py", line 10, in <module>
    from sina.util.cookies import cookies
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\cookies.py", line 158, in <module>
    cookie = processCookies(i['user'], i['pwd'])
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\cookies.py", line 137, in processCookies
    browser = webdriver.Chrome()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\selenium\webdriver\chrome\webdriver.py", line 68, in __init__
    self.service.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'chromedriver' executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home

2018-07-25 19:49:01 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: ['sina'])
2018-07-25 19:49:01 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.9.0, Python 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:04:45) [MSC v.1900 32 bit (Intel)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-07-25 19:49:01 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': ['sina'], 'COMMANDS_MODULE': 'sina.commands', 'DOWNLOAD_DELAY': 2, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_FILE': 'SinaSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'sina.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['sina.spiders']}
2018-07-25 19:49:01 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-07-25 19:49:01 [sina] INFO: Reading start URLs from redis key 'sinaspider:start_urls' (batch size: 16, encoding: utf-8
2018-07-25 19:49:01 [twisted] CRITICAL: Unhandled error in Deferred:
2018-07-25 19:49:01 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 80, in crawl
    self.engine = self._create_engine()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 105, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\downloader\__init__.py", line 88, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\misc.py", line 44, in load_object
    mod = import_module(module)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 994, in _gcd_import
  File "<frozen importlib._bootstrap>", line 971, in _find_and_load
  File "<frozen importlib._bootstrap>", line 955, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\middlewares.py", line 10, in <module>
    from sina.util.cookies import cookies
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\cookies.py", line 158, in <module>
    cookie = processCookies(i['user'], i['pwd'])
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\cookies.py", line 137, in processCookies
    browser = webdriver.Chrome()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\selenium\webdriver\chrome\webdriver.py", line 68, in __init__
    self.service.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'chromedriver' executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home

2018-07-25 19:49:01 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': ['sina'], 'COMMANDS_MODULE': 'sina.commands', 'DOWNLOAD_DELAY': 2, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_FILE': 'SinaSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'sina.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['sina.spiders']}
2018-07-25 19:49:01 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-07-25 19:49:01 [sina_fans] INFO: Reading start URLs from redis key 'SinaFansSpider:start_urls' (batch size: 16, encoding: utf-8
2018-07-25 19:49:01 [twisted] CRITICAL: Unhandled error in Deferred:
2018-07-25 19:49:01 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 80, in crawl
    self.engine = self._create_engine()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 105, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\downloader\__init__.py", line 88, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\misc.py", line 44, in load_object
    mod = import_module(module)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 994, in _gcd_import
  File "<frozen importlib._bootstrap>", line 971, in _find_and_load
  File "<frozen importlib._bootstrap>", line 955, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\middlewares.py", line 10, in <module>
    from sina.util.cookies import cookies
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\cookies.py", line 158, in <module>
    cookie = processCookies(i['user'], i['pwd'])
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\cookies.py", line 137, in processCookies
    browser = webdriver.Chrome()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\selenium\webdriver\chrome\webdriver.py", line 68, in __init__
    self.service.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'chromedriver' executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home

2018-07-25 19:49:58 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: ['sina'])
2018-07-25 19:49:58 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.9.0, Python 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:04:45) [MSC v.1900 32 bit (Intel)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-07-25 19:49:58 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': ['sina'], 'COMMANDS_MODULE': 'sina.commands', 'DOWNLOAD_DELAY': 2, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_FILE': 'SinaSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'sina.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['sina.spiders']}
2018-07-25 19:49:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-07-25 19:49:58 [sina] INFO: Reading start URLs from redis key 'sinaspider:start_urls' (batch size: 16, encoding: utf-8
2018-07-25 19:49:58 [twisted] CRITICAL: Unhandled error in Deferred:
2018-07-25 19:49:58 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 80, in crawl
    self.engine = self._create_engine()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 105, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\downloader\__init__.py", line 88, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\misc.py", line 44, in load_object
    mod = import_module(module)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 994, in _gcd_import
  File "<frozen importlib._bootstrap>", line 971, in _find_and_load
  File "<frozen importlib._bootstrap>", line 955, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\middlewares.py", line 10, in <module>
    from sina.util.cookies import cookies
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\cookies.py", line 158, in <module>
    cookie = processCookies(i['user'], i['pwd'])
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\cookies.py", line 137, in processCookies
    browser = webdriver.Chrome()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\selenium\webdriver\chrome\webdriver.py", line 68, in __init__
    self.service.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'chromedriver' executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home

2018-07-25 19:50:08 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: ['sina'])
2018-07-25 19:50:08 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.9.0, Python 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:04:45) [MSC v.1900 32 bit (Intel)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-07-25 19:50:08 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': ['sina'], 'COMMANDS_MODULE': 'sina.commands', 'DOWNLOAD_DELAY': 2, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_FILE': 'SinaSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'sina.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['sina.spiders']}
2018-07-25 19:50:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-07-25 19:50:09 [sina] INFO: Reading start URLs from redis key 'sinaspider:start_urls' (batch size: 16, encoding: utf-8
2018-07-25 19:50:09 [twisted] CRITICAL: Unhandled error in Deferred:
2018-07-25 19:50:09 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 80, in crawl
    self.engine = self._create_engine()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 105, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\downloader\__init__.py", line 88, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\misc.py", line 44, in load_object
    mod = import_module(module)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 994, in _gcd_import
  File "<frozen importlib._bootstrap>", line 971, in _find_and_load
  File "<frozen importlib._bootstrap>", line 955, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\middlewares.py", line 10, in <module>
    from sina.util.cookies import cookies
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\cookies.py", line 158, in <module>
    cookie = processCookies(i['user'], i['pwd'])
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\cookies.py", line 137, in processCookies
    browser = webdriver.Chrome()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\selenium\webdriver\chrome\webdriver.py", line 68, in __init__
    self.service.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'chromedriver' executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home

2018-07-25 19:54:21 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: ['sina'])
2018-07-25 19:54:21 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.9.0, Python 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:04:45) [MSC v.1900 32 bit (Intel)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-07-25 19:54:21 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': ['sina'], 'COMMANDS_MODULE': 'sina.commands', 'DOWNLOAD_DELAY': 2, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_FILE': 'SinaSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'sina.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['sina.spiders']}
2018-07-25 19:54:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-07-25 19:54:22 [sina] INFO: Reading start URLs from redis key 'SinaFansSpider:start_urls' (batch size: 16, encoding: utf-8
2018-07-25 19:54:22 [twisted] CRITICAL: Unhandled error in Deferred:
2018-07-25 19:54:22 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 80, in crawl
    self.engine = self._create_engine()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 105, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\downloader\__init__.py", line 88, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\misc.py", line 44, in load_object
    mod = import_module(module)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 994, in _gcd_import
  File "<frozen importlib._bootstrap>", line 971, in _find_and_load
  File "<frozen importlib._bootstrap>", line 955, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\middlewares.py", line 10, in <module>
    from sina.util.cookies import cookies
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\cookies.py", line 158, in <module>
    cookie = processCookies(i['user'], i['pwd'])
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\cookies.py", line 137, in processCookies
    browser = webdriver.Chrome()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\selenium\webdriver\chrome\webdriver.py", line 68, in __init__
    self.service.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'chromedriver' executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home

2018-07-25 19:55:16 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: ['sina'])
2018-07-25 19:55:16 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.9.0, Python 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:04:45) [MSC v.1900 32 bit (Intel)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-07-25 19:55:16 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': ['sina'], 'COMMANDS_MODULE': 'sina.commands', 'DOWNLOAD_DELAY': 2, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_FILE': 'SinaSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'sina.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['sina.spiders']}
2018-07-25 19:55:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-07-25 19:55:16 [sina] INFO: Reading start URLs from redis key 'SinaFansSpider:start_urls' (batch size: 16, encoding: utf-8
2018-07-25 19:55:16 [twisted] CRITICAL: Unhandled error in Deferred:
2018-07-25 19:55:16 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 80, in crawl
    self.engine = self._create_engine()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 105, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\downloader\__init__.py", line 88, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\misc.py", line 44, in load_object
    mod = import_module(module)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 994, in _gcd_import
  File "<frozen importlib._bootstrap>", line 971, in _find_and_load
  File "<frozen importlib._bootstrap>", line 955, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\middlewares.py", line 10, in <module>
    from sina.util.cookies import cookies
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\cookies.py", line 158, in <module>
    cookie = processCookies(i['user'], i['pwd'])
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\cookies.py", line 137, in processCookies
    browser = webdriver.Chrome()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\selenium\webdriver\chrome\webdriver.py", line 68, in __init__
    self.service.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'chromedriver' executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home

2018-07-25 21:23:35 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: ['sina'])
2018-07-25 21:23:35 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.9.0, Python 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:04:45) [MSC v.1900 32 bit (Intel)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-07-25 21:23:35 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': ['sina'], 'DOWNLOAD_DELAY': 2, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_FILE': 'SinaSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'sina.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['sina.spiders']}
2018-07-25 21:23:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-07-25 21:23:35 [sina] INFO: Reading start URLs from redis key 'SinaSpider:start_urls' (batch size: 16, encoding: utf-8
2018-07-25 21:24:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'sina.middlewares.SinaUseAgentChange',
 'sina.middlewares.SinaCookieChange',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'sina.middlewares.SinaDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-07-25 21:24:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-07-25 21:24:48 [scrapy.middleware] INFO: Enabled item pipelines:
['sina.pipelines.SinaPipeline', 'scrapy_redis.pipelines.RedisPipeline']
2018-07-25 21:24:48 [scrapy.core.engine] INFO: Spider opened
2018-07-25 21:24:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-07-25 21:24:48 [sina] INFO: Spider opened: sina
2018-07-25 21:24:49 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:24:52 [scrapy.core.scraper] ERROR: Error processing {'blogger': 2517436943,
 'fansTotal': '粉丝391人',
 'headshot': 'http://tva2.sinaimg.cn/crop.0.0.100.100.50/006jaVDxjw8eyn01ynbw7j302s02sjr7.jpg',
 'name': '股市Atina'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:52 [scrapy.core.scraper] ERROR: Error processing {'blogger': 2517436943,
 'fansTotal': '粉丝871人',
 'headshot': 'http://tvax2.sinaimg.cn/crop.0.0.1242.1242.50/98fb3213ly8fqey9hcdfsj20yi0yiafw.jpg',
 'name': '蔡蕊蔡蕊蔡蔡蔡蔡蕊哦蔡蕊'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:52 [scrapy.core.scraper] ERROR: Error processing {'blogger': 2517436943,
 'fansTotal': '粉丝175人',
 'headshot': 'http://tva2.sinaimg.cn/crop.0.0.301.301.50/006ko46Fjw1ez4dt4xz1ij308d08d0t5.jpg',
 'name': '念之小萌萌'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:52 [scrapy.core.scraper] ERROR: Error processing {'blogger': 2517436943,
 'fansTotal': '粉丝286人',
 'headshot': 'http://tva1.sinaimg.cn/crop.15.59.185.185.50/0065dME7jw8exp304glyoj308d09aq31.jpg',
 'name': '不喜争宠的刘精英'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:52 [scrapy.core.scraper] ERROR: Error processing {'blogger': 2517436943,
 'fansTotal': '粉丝827人',
 'headshot': 'http://tva4.sinaimg.cn/crop.41.28.92.92.50/0065DJpljw8eyuo1kiacsj304n0463yh.jpg',
 'name': '股惑人升'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.cn/2517436943/fans?page=2> (referer: https://weibo.cn/2517436943/fans)
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
GeneratorExit
2018-07-25 21:24:52 [scrapy.core.scraper] ERROR: Error processing {'blogger': 2517436943,
 'fansTotal': '粉丝355人',
 'headshot': 'http://tvax3.sinaimg.cn/crop.0.0.996.996.50/005yQN32ly8fhp92xfzrrj30ro0ro762.jpg',
 'name': '秋叶静美125232369'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:52 [twisted] CRITICAL: Unhandled error in Deferred:
2018-07-25 21:24:52 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\task.py", line 517, in _oneWorkUnit
    result = next(self._iterator)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\defer.py", line 63, in <genexpr>
    work = (callable(elem, *args, **named) for elem in iterable)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\scraper.py", line 183, in _process_spidermw_output
    self.crawler.engine.crawl(request=output, spider=spider)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 210, in crawl
    self.schedule(request, spider)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 216, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 162, in enqueue_request
    if not request.dont_filter and self.df.request_seen(request):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\dupefilter.py", line 100, in request_seen
    added = self.server.sadd(self.key, fp)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1600, in sadd
    return self.execute_command('SADD', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:52 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:24:52 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:24:52 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5235640836,
 'fansTotal': '粉丝12人',
 'headshot': 'http://tva4.sinaimg.cn/crop.0.0.1006.1006.50/006Idq7Xjw8fcvydos4wsj30ry0ry765.jpg',
 'name': 'XUYUANYUAN随风行走'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:52 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5235640836,
 'fansTotal': '粉丝213人',
 'headshot': 'http://tvax2.sinaimg.cn/crop.0.0.996.996.50/006Rnzf3ly8fgys59lqtqj30ro0roadq.jpg',
 'name': '刘福海洋1'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:52 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5235640836,
 'fansTotal': '粉丝27人',
 'headshot': 'http://tvax4.sinaimg.cn/crop.0.0.996.996.50/005LiDpsly8fq4divxk8cj30ro0rojwl.jpg',
 'name': '重庆冰糕不是雪糕'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:52 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5235640836,
 'fansTotal': '粉丝40人',
 'headshot': 'http://tva2.sinaimg.cn/crop.0.0.180.180.50/68e51c11jw1e8qgp5bmzyj2050050aa8.jpg',
 'name': 'RaindayJin'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:52 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5235640836,
 'fansTotal': '粉丝16人',
 'headshot': 'http://tva2.sinaimg.cn/crop.0.0.664.664.50/9a74a93fjw8f46p33p8uuj20ig0ig0um.jpg',
 'name': '愿你之名冠我之姓'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:52 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5235640836,
 'fansTotal': '粉丝170人',
 'headshot': 'http://tva1.sinaimg.cn/crop.0.0.180.180.50/49256baajw1e8qgp5bmzyj2050050aa8.jpg',
 'name': '佐手卡卡'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.cn/5235640836/fans> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
GeneratorExit
2018-07-25 21:24:52 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5235640836,
 'fansTotal': '粉丝255人',
 'headshot': 'http://tva1.sinaimg.cn/crop.0.0.290.290.50/ec6c6615jw8ecl08oeh80j2082082dfr.jpg',
 'name': '万事皆浮云_'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:52 [twisted] CRITICAL: Unhandled error in Deferred:
2018-07-25 21:24:52 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\task.py", line 517, in _oneWorkUnit
    result = next(self._iterator)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\defer.py", line 63, in <genexpr>
    work = (callable(elem, *args, **named) for elem in iterable)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\scraper.py", line 183, in _process_spidermw_output
    self.crawler.engine.crawl(request=output, spider=spider)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 210, in crawl
    self.schedule(request, spider)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 216, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 162, in enqueue_request
    if not request.dont_filter and self.df.request_seen(request):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\dupefilter.py", line 100, in request_seen
    added = self.server.sadd(self.key, fp)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1600, in sadd
    return self.execute_command('SADD', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:52 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:24:53 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:24:54 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:24:54 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5235640836, 'group': '本地生活[2]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:54 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5235640836, 'group': '搞笑幽默[0]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:54 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5235640836, 'group': '视频音乐[0]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:54 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5235640836, 'group': '旅游[0]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:54 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5235640836, 'group': '校园生活[0]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:55 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5235640836, 'group': '新闻趣事[0]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:55 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5235640836, 'group': '美食[0]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:55 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5235640836, 'group': 'IT数码[0]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:55 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5235640836, 'group': '宠物[0]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:55 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5235640836, 'group': '美图[0]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:55 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5235640836, 'group': '情感生活[0]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:55 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:24:57 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:24:57 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5235640836,
 'comeFrom': '2016-03-13 10:50:07&nbsp;来自手机微博触屏版',
 'commentNumber': '评论[0]',
 'content': '【心理箴言】现实是污浊的河流，要想接受污浊的河流而自身不被污染，我们必须成为大海。 \u200b\u200b\u200b',
 'goodNumber': '赞[9]',
 'transmitNumber': '转发[0]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:57 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5235640836,
 'comeFrom': '2016-03-12 13:41:45&nbsp;来自微博 weibo.com',
 'commentNumber': '评论[0]',
 'content': '【趣味测试】下图中第一眼看到的三个词就是对你的描述哟~应该蛮准的。 \u200b\u200b\u200b',
 'goodNumber': '赞[0]',
 'transmitNumber': '转发[0]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:57 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5235640836,
 'comeFrom': '2016-03-11 22:22:36&nbsp;来自微博 weibo.com',
 'commentNumber': '评论[0]',
 'content': '一个成熟的人，应该多点宽容与理解。对于过去不理解的，以后会理解，小时候不理解的，长大了会理解，长大了不理解的，年老了会理解。无论如何也理解不了的呢？只要人家没违法，我们就要理解其存在的合理性并容许别人以其喜欢的方式自由存在。理解别人，就是理解自己。 '
            '\u200b\u200b\u200b',
 'goodNumber': '赞[0]',
 'transmitNumber': '转发[0]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:57 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5235640836,
 'comeFrom': '2016-03-11 08:52:38&nbsp;来自手机微博触屏版',
 'commentNumber': '评论[0]',
 'content': '做你自己喜欢的事，哪怕别人都笑你傻，你也完全可以全身心地去做，只要你喜欢，只要做这件事会让你快乐！只有你的快乐才是你该在乎的；别怕别人说什么，除了帮你快乐的话，你都可以不听。这样你的心就简单了，事就简单了，你的快乐就多了。[哈哈] '
            '\u200b\u200b\u200b',
 'goodNumber': '赞[0]',
 'transmitNumber': '转发[0]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:57 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5235640836,
 'comeFrom': '2016-03-10 13:31:24&nbsp;来自手机微博触屏版',
 'commentNumber': '评论[0]',
 'content': '我是要为人民服务，但不可能为所有的人、所有的民服务；我要为我的亲人、友人、熟人、美人这样的“民”服务，光这些就够我忙活的了，哪还有精力再为其它的人其它的民服务呢？ '
            '\u200b\u200b\u200b',
 'goodNumber': '赞[0]',
 'transmitNumber': '转发[0]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:57 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5235640836,
 'comeFrom': '2016-03-09 23:16:05&nbsp;来自微博 weibo.com',
 'commentNumber': '评论[0]',
 'content': '人生的路走走停停是一种闲适，边走边看是一种优雅，边走边忘是一种豁达。人生艰难，但是如果你有笑对人生的能力，你就有享受人生的能力。我们曾如此渴望命运的波澜，到最后才发现：人生最曼妙的风景，竟是内心的淡定与从容。我们曾如此期盼外界的认可，到最后才知道：世界是自己的，与他人毫无关系 '
            '\u200b\u200b\u200b',
 'goodNumber': '赞[0]',
 'transmitNumber': '转发[0]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:57 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5235640836,
 'comeFrom': '2016-03-09 10:10:24&nbsp;来自手机微博触屏版',
 'commentNumber': '评论[0]',
 'content': '总有被误解的时候，淡淡一笑；总有委屈的时候，坦然一笑，心大了，乐多了；心小了，愁多了，生活中不能事事顺心，人生中不会处处如意，或许有些烦恼是别人给的，有些愁绪是自己找的，不管怎样来的，不论如何得的，看淡了，坦然了，给心一个安乐，给情一个安宁，生活，多了开心，人生，就多了乐趣。 '
            '\u200b\u200b\u200b',
 'goodNumber': '赞[0]',
 'transmitNumber': '转发[0]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:57 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5235640836,
 'comeFrom': '2016-03-08 14:39:56&nbsp;来自手机微博触屏版',
 'commentNumber': '转发[0]',
 'content': '三伏天了，啥叫三伏？当然是幸福对你贴贴“伏伏”，好运被你轻松降“伏”，吉祥也来乖乖臣“伏”。三伏天，愿你三伏清凉好心情！  '
            '我在这里：  ',
 'goodNumber': '显示地图',
 'transmitNumber': '赞[0]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:57 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5235640836,
 'comeFrom': '2016-03-07 23:54:51&nbsp;来自微博 weibo.com',
 'commentNumber': '评论[0]',
 'content': '告别单身，马上就加入到腾讯女性 \u200b\u200b\u200b',
 'goodNumber': '赞[3]',
 'transmitNumber': '转发[1]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.cn/5235640836/profile?filter=1&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
GeneratorExit
2018-07-25 21:24:57 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5235640836,
 'comeFrom': '2016-03-07 08:51:52&nbsp;来自微博 weibo.com',
 'commentNumber': '评论[0]',
 'content': '世上或许有一段不可替代的感情，却没有一个人是不可替代的。[yz砸] \u200b\u200b\u200b',
 'goodNumber': '赞[3]',
 'transmitNumber': '转发[0]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:57 [twisted] CRITICAL: Unhandled error in Deferred:
2018-07-25 21:24:57 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\task.py", line 517, in _oneWorkUnit
    result = next(self._iterator)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\defer.py", line 63, in <genexpr>
    work = (callable(elem, *args, **named) for elem in iterable)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\scraper.py", line 183, in _process_spidermw_output
    self.crawler.engine.crawl(request=output, spider=spider)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 210, in crawl
    self.schedule(request, spider)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 216, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 162, in enqueue_request
    if not request.dont_filter and self.df.request_seen(request):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\dupefilter.py", line 100, in request_seen
    added = self.server.sadd(self.key, fp)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1600, in sadd
    return self.execute_command('SADD', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:57 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:24:58 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:24:59 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:24:59 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5676304901,
 'fansTotal': '粉丝120人',
 'headshot': 'http://tva2.sinaimg.cn/crop.0.6.499.499.50/9bbb249bjw8f8bin7hld1j20dv0e8jrp.jpg',
 'name': '久闻窿--屎劲'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:59 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5676304901,
 'fansTotal': '粉丝238人',
 'headshot': 'http://tvax3.sinaimg.cn/crop.0.0.996.996.50/c5ecfa17ly8fgpud6yi25j20ro0roq5d.jpg',
 'name': '希尔瑞厄斯'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:59 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5676304901,
 'fansTotal': '粉丝162人',
 'headshot': 'http://tva2.sinaimg.cn/crop.0.0.640.640.50/be70814ejw8f280dxvffzj20hs0hs74x.jpg',
 'name': '奋斗中的严大大'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:59 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5676304901,
 'fansTotal': '粉丝173人',
 'headshot': 'http://tva2.sinaimg.cn/crop.0.0.664.664.50/0062faS1jw8f64rmv69zpj30ig0iggmz.jpg',
 'name': '心不动啊则不痛'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:59 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5676304901,
 'fansTotal': '粉丝37人',
 'headshot': 'http://tvax2.sinaimg.cn/crop.0.0.512.512.50/0069t7Zcly8frm7ikrgdkj30e80e8q3i.jpg',
 'name': '苕-苕'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:59 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5676304901,
 'fansTotal': '粉丝62人',
 'headshot': 'http://tva2.sinaimg.cn/crop.0.0.180.180.50/6b2564e9jw1e8qgp5bmzyj2050050aa8.jpg',
 'name': 'Spand-a</a><img '
         'src="https://h5.sinaimg.cn/upload/2016/05/26/319/5547.gif" '
         'alt="达人"/><br/>粉丝811人<br/><a '
         'href="https://weibo.cn/attention/add?uid=1797612777&amp;rl=1&amp;st=b8d34c">关注他</a></td></tr></table> '
         '<div class="s"></div><table><tr><td valign="top" style="width: '
         '52px"><a href="https://weibo.cn/u/5541458613"><img '
         'src="http://tvax3.sinaimg.cn/crop.0.0.667.667.50/00631owJly8fthqzjbs19j30ij0ijt9p.jpg" '
         'alt="pic" /></a></td><td valign="top"><a '
         'href="https://weibo.cn/u/5541458613">felixorange-'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 19, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'name' at row 1")
2018-07-25 21:24:59 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5676304901,
 'fansTotal': '粉丝132人',
 'headshot': 'http://tvax3.sinaimg.cn/crop.0.0.1010.1010.50/d89a8faely8fq5on9w9kuj20s20s2tcq.jpg',
 'name': 'Tinychan陈少聪'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:59 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5676304901,
 'fansTotal': '粉丝91人',
 'headshot': 'http://tvax4.sinaimg.cn/crop.538.0.295.295.50/005zfBHyly8ftda2wm2d0j311y0lcjx4.jpg',
 'name': 'Mnemosyne_18'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.cn/5676304901/fans> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
GeneratorExit
2018-07-25 21:24:59 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5676304901,
 'fansTotal': '粉丝227人',
 'headshot': 'http://tva4.sinaimg.cn/crop.0.0.180.180.50/6b0171a5jw1e8qgp5bmzyj2050050aa8.jpg',
 'name': '方大妹紙的小跑腿'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:59 [twisted] CRITICAL: Unhandled error in Deferred:
2018-07-25 21:24:59 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\task.py", line 517, in _oneWorkUnit
    result = next(self._iterator)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\defer.py", line 63, in <genexpr>
    work = (callable(elem, *args, **named) for elem in iterable)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\scraper.py", line 183, in _process_spidermw_output
    self.crawler.engine.crawl(request=output, spider=spider)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 210, in crawl
    self.schedule(request, spider)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 216, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 162, in enqueue_request
    if not request.dont_filter and self.df.request_seen(request):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\dupefilter.py", line 100, in request_seen
    added = self.server.sadd(self.key, fp)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1600, in sadd
    return self.execute_command('SADD', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:24:59 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:02 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:02 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5676304901, 'group': '搞笑幽默[0]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:02 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:03 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:05 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:05 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5676304901,
 'comeFrom': '02月09日 08:33&nbsp;来自<a '
             'href="https://weibo.cn/sinaurl?f=w&amp;u=http%3A%2F%2Fdown.sina.cn%2Fsinaclient%2Fweibo%2Findex%2Fgsid%2F0%2Fmid%2F0">iPhone客户端</a>',
 'commentNumber': '赞[5]',
 'content': '【骑士三方交易评级：胡德希尔能帮骑士争冠吗】',
 'goodNumber': '<img '
               'src="http://wx4.sinaimg.cn/wap180/006c9ccRgy1fo9w73o9kdj30c8062dge.jpg" '
               'alt="图片" class="ib" />',
 'transmitNumber': '原图'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'comeFrom' at row 1")
2018-07-25 21:25:05 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5676304901,
 'comeFrom': '02月09日 08:16&nbsp;来自<a '
             'href="https://weibo.cn/sinaurl?f=w&amp;u=http%3A%2F%2Fdown.sina.cn%2Fsinaclient%2Fweibo%2Findex%2Fgsid%2F0%2Fmid%2F0">iPhone客户端</a>',
 'commentNumber': '评论[0]',
 'content': '骑士。。',
 'goodNumber': '赞[7]',
 'transmitNumber': '转发[0]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'comeFrom' at row 1")
2018-07-25 21:25:05 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5676304901,
 'comeFrom': '02月07日 10:41&nbsp;来自微博 weibo.com',
 'commentNumber': '赞[6]',
 'content': '我发起了一个投票 【你最喜欢的国内篮球解说是？】',
 'goodNumber': '<img '
               'src="http://wx1.sinaimg.cn/wap180/006c9ccRgy1fo7ooumkmpj30fa08cjt1.jpg" '
               'alt="图片" class="ib" />',
 'transmitNumber': '原图'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'goodNumber' at row 1")
2018-07-25 21:25:05 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5676304901,
 'comeFrom': '02月06日 16:45&nbsp;来自微博 weibo.com',
 'commentNumber': '赞[3]',
 'content': '有点萌。 \u200b\u200b\u200b',
 'goodNumber': '<img '
               'src="http://wx3.sinaimg.cn/wap180/006c9ccRgy1fo6tkrvjwuj30dg0dm0tg.jpg" '
               'alt="图片" class="ib" />',
 'transmitNumber': '原图'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'goodNumber' at row 1")
2018-07-25 21:25:05 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5676304901,
 'comeFrom': '02月06日 12:28&nbsp;来自微博 weibo.com',
 'commentNumber': '原图',
 'content': '爵士今天客场133-109大胜鹈鹕，据统计自戈贝尔复出后，爵士百回合净胜10.9分(全联盟第一），投篮命中率50.5%(全联盟第二），取得7胜2负的战绩(全联盟第二）。',
 'goodNumber': '全文',
 'transmitNumber': '<img '
                   'src="http://wx3.sinaimg.cn/wap180/006c9ccRgy1fo6m53a5msj30sg0g0dgg.jpg" '
                   'alt="图片" class="ib" />'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'transmitNumber' at row 1")
2018-07-25 21:25:05 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5676304901,
 'comeFrom': '02月06日 12:18&nbsp;来自微博 weibo.com',
 'commentNumber': '赞[4]',
 'content': '洛佩斯这是？ \u200b\u200b\u200b',
 'goodNumber': '<img '
               'src="http://wx2.sinaimg.cn/wap180/006c9ccRgy1fo6lurdvqug308c08dhdt.gif" '
               'alt="图片" class="ib" />',
 'transmitNumber': '原图'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'goodNumber' at row 1")
2018-07-25 21:25:05 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5676304901,
 'comeFrom': '02月06日 11:53&nbsp;来自微博 weibo.com',
 'commentNumber': '赞[7]',
 'content': '勇士询价布拉德利？ \u200b\u200b\u200b',
 'goodNumber': '<img '
               'src="http://wx2.sinaimg.cn/wap180/006c9ccRgy1fo6l50468kj30j90clqeb.jpg" '
               'alt="图片" class="ib" />',
 'transmitNumber': '原图'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'goodNumber' at row 1")
2018-07-25 21:25:05 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5676304901,
 'comeFrom': '02月05日 16:56&nbsp;来自微博 weibo.com',
 'commentNumber': '赞[4]',
 'content': '泡椒你这是？ \u200b\u200b\u200b',
 'goodNumber': '<img '
               'src="http://wx3.sinaimg.cn/wap180/006c9ccRgy1fo5oa3phakj30fn0fiabf.jpg" '
               'alt="图片" class="ib" />',
 'transmitNumber': '原图'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'goodNumber' at row 1")
2018-07-25 21:25:05 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5676304901,
 'comeFrom': '02月05日 13:56&nbsp;来自微博 weibo.com',
 'commentNumber': '赞[12]',
 'content': '老鹰球迷唐斯。[哈哈]',
 'goodNumber': '<img '
               'src="http://wx3.sinaimg.cn/wap180/006c9ccRgy1fo5j2m9ip7j30m80bowga.jpg" '
               'alt="图片" class="ib" />',
 'transmitNumber': '原图'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'goodNumber' at row 1")
2018-07-25 21:25:05 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5676304901,
 'comeFrom': '02月05日 12:36&nbsp;来自微博 weibo.com',
 'commentNumber': '赞[2]',
 'content': '这要是走步。。 \u200b\u200b\u200b',
 'goodNumber': '<img '
               'src="http://wx2.sinaimg.cn/wap180/006c9ccRgy1fo5gromgu5g307z05j4qq.gif" '
               'alt="图片" class="ib" />',
 'transmitNumber': '原图'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'goodNumber' at row 1")
2018-07-25 21:25:05 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:08 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:08 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:09 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5871897095,
 'fansTotal': '粉丝3人',
 'headshot': 'http://tvax3.sinaimg.cn/crop.0.0.664.664.50/007cT7Rply8ftgotpwpbqj30ig0ig75p.jpg',
 'name': '别当我不存在訷'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:09 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5871897095,
 'fansTotal': '粉丝3人',
 'headshot': 'http://tvax3.sinaimg.cn/crop.0.0.664.664.50/007cHciOly8ft9h5z5gvqj30ig0ig75c.jpg',
 'name': '巢莘昝惭'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:09 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5871897095,
 'fansTotal': '粉丝11人',
 'headshot': 'http://tvax3.sinaimg.cn/crop.0.0.664.664.50/006PBlp2ly8ft7i18yasxj30ig0igtad.jpg',
 'name': '巴黎塔的那场婚礼結'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:09 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5871897095,
 'fansTotal': '粉丝9人',
 'headshot': 'http://tvax1.sinaimg.cn/crop.0.0.664.664.50/007cblCBly8ft8ma7sm03j30ig0igjso.jpg',
 'name': '你怎知我情深靹'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:09 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5871897095,
 'fansTotal': '粉丝3人',
 'headshot': 'http://tvax4.sinaimg.cn/crop.0.0.664.664.50/007cblAUly8ft5o7bnk9vj30ig0igjso.jpg',
 'name': '卞戚充咕'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:09 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5871897095,
 'fansTotal': '粉丝4人',
 'headshot': 'http://tvax3.sinaimg.cn/crop.0.0.664.664.50/007caLculy8ft30u6fnovj30ig0ig75j.jpg',
 'name': '用户6592810578'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:09 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.cn/5871897095/fans> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
GeneratorExit
2018-07-25 21:25:09 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5871897095,
 'fansTotal': '粉丝4人',
 'headshot': 'http://tvax3.sinaimg.cn/default/images/default_avatar_female_50.gif',
 'name': '用户6581368378'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:09 [twisted] CRITICAL: Unhandled error in Deferred:
2018-07-25 21:25:09 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\task.py", line 517, in _oneWorkUnit
    result = next(self._iterator)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\defer.py", line 63, in <genexpr>
    work = (callable(elem, *args, **named) for elem in iterable)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\scraper.py", line 183, in _process_spidermw_output
    self.crawler.engine.crawl(request=output, spider=spider)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 210, in crawl
    self.schedule(request, spider)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 216, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 162, in enqueue_request
    if not request.dont_filter and self.df.request_seen(request):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\dupefilter.py", line 100, in request_seen
    added = self.server.sadd(self.key, fp)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1600, in sadd
    return self.execute_command('SADD', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:09 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:10 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:11 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5871897095, 'group': '本地生活[11]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:11 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5871897095, 'group': '旅游[0]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:11 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5871897095, 'group': '美食[3]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:11 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5871897095, 'group': '房产家居[0]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:11 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5871897095, 'group': '车世界[0]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:11 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:13 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:13 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:14 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5871897095,
 'comeFrom': '04月06日 19:45&nbsp;来自小米6 拍人更美',
 'commentNumber': '原图',
 'content': '唯有美食与美女不可辜负！ \u200b\u200b\u200b',
 'goodNumber': '组图共9张',
 'transmitNumber': '<img '
                   'src="http://wx2.sinaimg.cn/wap180/006pnSFFgy1fq36bs6s0jj30zn0qowm8.jpg" '
                   'alt="图片" class="ib" />'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'transmitNumber' at row 1")
2018-07-25 21:25:14 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5871897095,
 'comeFrom': '06月14日 00:46&nbsp;来自生日动态',
 'commentNumber': '评论[0]',
 'content': '',
 'goodNumber': '赞[0]',
 'transmitNumber': '转发[0]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:14 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5871897095,
 'comeFrom': '04月03日 21:35&nbsp;来自小米6 拍人更美',
 'commentNumber': '原图',
 'content': '每晚都需要抱着去看看外面的灯红酒绿。 \u200b\u200b\u200b',
 'goodNumber': '组图共9张',
 'transmitNumber': '<img '
                   'src="http://wx1.sinaimg.cn/wap180/006pnSFFgy1fpzsn3r2vaj30qo1ben2y.jpg" '
                   'alt="图片" class="ib" />'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'transmitNumber' at row 1")
2018-07-25 21:25:14 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5871897095,
 'comeFrom': '03月26日 00:02&nbsp;来自小米6 拍人更美',
 'commentNumber': '原图',
 'content': '小情人[太开心][太开心] \u200b\u200b\u200b',
 'goodNumber': '组图共5张',
 'transmitNumber': '<img '
                   'src="http://wx3.sinaimg.cn/wap180/006pnSFFgy1fppiamqmknj31400qoq6q.jpg" '
                   'alt="图片" class="ib" />'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'transmitNumber' at row 1")
2018-07-25 21:25:14 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5871897095,
 'comeFrom': '02月20日 20:17&nbsp;来自粉丝红包',
 'commentNumber': '赞[0]',
 'content': '',
 'goodNumber': '@陈欧',
 'transmitNumber': '努力努力再努力x 的红包'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'transmitNumber' at row 1")
2018-07-25 21:25:14 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5871897095,
 'comeFrom': '02月20日 20:17&nbsp;来自粉丝红包',
 'commentNumber': '赞[0]',
 'content': '',
 'goodNumber': '@微博会员',
 'transmitNumber': '彭于晏 的红包'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:14 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5871897095,
 'comeFrom': '01月06日 23:13&nbsp;来自手机酷狗安卓版',
 'commentNumber': '原图',
 'content': '我正在听刘德华的歌曲《谢谢你的爱 (国语)》（来自',
 'goodNumber': 'http://t.cn/RHrrFMu',
 'transmitNumber': '<img '
                   'src="http://wx3.sinaimg.cn/wap180/006pnSFFgy1fn7aleratcj30dc0dc755.jpg" '
                   'alt="图片" class="ib" />'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'goodNumber' at row 1")
2018-07-25 21:25:14 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5871897095,
 'comeFrom': '02月10日 22:31&nbsp;来自小米6 拍人更美',
 'commentNumber': '显示地图',
 'content': '年，就这样快到了，心，却依旧，像平静的湖面，无波无澜。从什么时候开始，过年，不再是一种憧憬，而成为了一种形式。 '
            '以往过年，缺的是年货，不缺年味；现在',
 'goodNumber': '#晒年味#',
 'transmitNumber': '全文'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:14 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5871897095,
 'comeFrom': '2017-12-24 06:52:22&nbsp;来自微博活动',
 'commentNumber': '转发[0]',
 'content': '',
 'goodNumber': '下微博赢小米MIX手机',
 'transmitNumber': '赞[0]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:14 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5871897095,
 'comeFrom': '2017-11-28 21:50:26&nbsp;来自小米6 拍人更美',
 'commentNumber': '赞[2]',
 'content': '我发表了头条文章:《宝贝乖》 ',
 'goodNumber': '南充·火花街区',
 'transmitNumber': '显示地图'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:14 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:16 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:16 [scrapy.core.scraper] ERROR: Error processing {'blogger': 2139359753,
 'fansTotal': '粉丝79人',
 'headshot': 'http://tvax1.sinaimg.cn/crop.0.0.512.512.50/005DzTWhly8fqi91iyzcbj30e80e8754.jpg',
 'name': '曲韵浓丶'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:16 [scrapy.core.scraper] ERROR: Error processing {'blogger': 2139359753,
 'fansTotal': '粉丝28人',
 'headshot': 'http://tvax2.sinaimg.cn/crop.0.0.664.664.50/0078CiFLly8ft9q4ihwutj30ig0igjs0.jpg',
 'name': '易大佬娇妻zhy'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:16 [scrapy.core.scraper] ERROR: Error processing {'blogger': 2139359753,
 'fansTotal': '粉丝6人',
 'headshot': 'http://tvax2.sinaimg.cn/crop.0.0.996.996.50/006Tnk6wly8fnzukv3dt6j30ro0rowj9.jpg',
 'name': '不爱你的人拿什么感动'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:16 [scrapy.core.scraper] ERROR: Error processing {'blogger': 2139359753,
 'fansTotal': '粉丝73人',
 'headshot': 'http://tva2.sinaimg.cn/crop.0.1.640.640.50/005Oc8RVjw8f1a76ta94qj30hs0huq35.jpg',
 'name': '張張張-張小米'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:16 [scrapy.core.scraper] ERROR: Error processing {'blogger': 2139359753,
 'fansTotal': '粉丝4人',
 'headshot': 'http://tvax2.sinaimg.cn/crop.0.0.1242.1242.50/494f4d8bly8fphxsznf6cj20yi0yiwhx.jpg',
 'name': '一只有梦想的烤盐兔'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:16 [scrapy.core.scraper] ERROR: Error processing {'blogger': 2139359753,
 'fansTotal': '粉丝33人',
 'headshot': 'http://tva2.sinaimg.cn/crop.1.0.638.638.50/005vbu5yjw8ekui81b89vj30hs0hq74w.jpg',
 'name': 'Xiuxiu-Xi'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:16 [scrapy.core.scraper] ERROR: Error processing {'blogger': 2139359753,
 'fansTotal': '粉丝39人',
 'headshot': 'http://tvax1.sinaimg.cn/crop.0.0.750.750.50/005Po3Eely8fj4x01mk9hj30ku0kugmg.jpg',
 'name': '团子大师'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:16 [scrapy.core.scraper] ERROR: Error processing {'blogger': 2139359753,
 'fansTotal': '粉丝108人',
 'headshot': 'http://tvax1.sinaimg.cn/crop.0.0.996.996.50/e15306f0ly8foxm7mi41yj20ro0rp3zb.jpg',
 'name': '神的少女LAY'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.cn/2139359753/fans> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
GeneratorExit
2018-07-25 21:25:16 [scrapy.core.scraper] ERROR: Error processing {'blogger': 2139359753,
 'fansTotal': '粉丝19人',
 'headshot': 'http://tvax1.sinaimg.cn/crop.0.0.400.400.50/005ZkVmZly8fmy290s0a8j30b40b4js0.jpg',
 'name': '人生苦短嫌命长'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:16 [scrapy.core.scraper] ERROR: Error processing {'blogger': 2139359753,
 'fansTotal': '粉丝161人',
 'headshot': 'http://tvax4.sinaimg.cn/crop.0.0.996.996.50/006o5e1mly8frrfzb80rlj30ro0rotci.jpg',
 'name': 'Sudamasaki__'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:16 [twisted] CRITICAL: Unhandled error in Deferred:
2018-07-25 21:25:16 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\task.py", line 517, in _oneWorkUnit
    result = next(self._iterator)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\defer.py", line 63, in <genexpr>
    work = (callable(elem, *args, **named) for elem in iterable)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\scraper.py", line 183, in _process_spidermw_output
    self.crawler.engine.crawl(request=output, spider=spider)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 210, in crawl
    self.schedule(request, spider)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 216, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 162, in enqueue_request
    if not request.dont_filter and self.df.request_seen(request):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\dupefilter.py", line 100, in request_seen
    added = self.server.sadd(self.key, fp)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1600, in sadd
    return self.execute_command('SADD', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:16 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:18 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:19 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:19 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:20 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:20 [scrapy.core.scraper] ERROR: Error processing {'blogger': 2139359753,
 'comeFrom': '今天 08:00&nbsp;来自微博 weibo.com',
 'commentNumber': '赞[159]',
 'content': '',
 'goodNumber': '<img '
               'src="http://wx2.sinaimg.cn/wap180/7f840a09gy1ftl4pag315j20f00qo4qp.jpg" '
               'alt="图片" class="ib" />',
 'transmitNumber': '原图'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'goodNumber' at row 1")
2018-07-25 21:25:20 [scrapy.core.scraper] ERROR: Error processing {'blogger': 2139359753,
 'comeFrom': '07月24日 22:00&nbsp;来自微博 weibo.com',
 'commentNumber': '赞[63]',
 'content': '',
 'goodNumber': '<img '
               'src="http://wx4.sinaimg.cn/wap180/7f840a09gy1ftjpmd34xtj20ku0kudmt.jpg" '
               'alt="图片" class="ib" />',
 'transmitNumber': '原图'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'goodNumber' at row 1")
2018-07-25 21:25:20 [scrapy.core.scraper] ERROR: Error processing {'blogger': 2139359753,
 'comeFrom': '07月24日 21:30&nbsp;来自微博 weibo.com',
 'commentNumber': '赞[12]',
 'content': '',
 'goodNumber': '<img '
               'src="http://wx1.sinaimg.cn/wap180/7f840a09gy1ftkxfazwsnj20ku0tg40x.jpg" '
               'alt="图片" class="ib" />',
 'transmitNumber': '原图'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'goodNumber' at row 1")
2018-07-25 21:25:20 [scrapy.core.scraper] ERROR: Error processing {'blogger': 2139359753,
 'comeFrom': '07月24日 08:00&nbsp;来自微博 weibo.com',
 'commentNumber': '赞[137]',
 'content': '',
 'goodNumber': '<img '
               'src="http://wx1.sinaimg.cn/wap180/7f840a09gy1ftjyfervcej20f00qoqgm.jpg" '
               'alt="图片" class="ib" />',
 'transmitNumber': '原图'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'goodNumber' at row 1")
2018-07-25 21:25:20 [scrapy.core.scraper] ERROR: Error processing {'blogger': 2139359753,
 'comeFrom': '07月23日 22:00&nbsp;来自微博 weibo.com',
 'commentNumber': '赞[139]',
 'content': '',
 'goodNumber': '<img '
               'src="http://wx1.sinaimg.cn/wap180/7f840a09gy1ftgdhdymnyj20m80hstmj.jpg" '
               'alt="图片" class="ib" />',
 'transmitNumber': '原图'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'goodNumber' at row 1")
2018-07-25 21:25:20 [scrapy.core.scraper] ERROR: Error processing {'blogger': 2139359753,
 'comeFrom': '07月23日 21:30&nbsp;来自微博 weibo.com',
 'commentNumber': '赞[67]',
 'content': '',
 'goodNumber': '<img '
               'src="http://wx1.sinaimg.cn/wap180/7f840a09gy1ftg5fibuspj21kw1kwe81.jpg" '
               'alt="图片" class="ib" />',
 'transmitNumber': '原图'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'goodNumber' at row 1")
2018-07-25 21:25:20 [scrapy.core.scraper] ERROR: Error processing {'blogger': 2139359753,
 'comeFrom': '07月23日 18:29&nbsp;来自专业版微博',
 'commentNumber': '原图',
 'content': '枯燥的知识千篇一律，有趣的内容万里挑一！',
 'goodNumber': '组图共6张',
 'transmitNumber': '<img '
                   'src="http://wx1.sinaimg.cn/wap180/7f840a09gy1ftjz106c93j20m80vuajr.jpg" '
                   'alt="图片" class="ib" />'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'transmitNumber' at row 1")
2018-07-25 21:25:20 [scrapy.core.scraper] ERROR: Error processing {'blogger': 2139359753,
 'comeFrom': '07月23日 08:00&nbsp;来自微博 weibo.com',
 'commentNumber': '赞[234]',
 'content': '',
 'goodNumber': '<img '
               'src="http://wx3.sinaimg.cn/wap180/7f840a09gy1ftge23xnhaj20f00qowwn.jpg" '
               'alt="图片" class="ib" />',
 'transmitNumber': '原图'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'goodNumber' at row 1")
2018-07-25 21:25:20 [scrapy.core.scraper] ERROR: Error processing {'blogger': 2139359753,
 'comeFrom': '07月22日 21:30&nbsp;来自微博 weibo.com',
 'commentNumber': '原图',
 'content': '',
 'goodNumber': '组图共2张',
 'transmitNumber': '<img '
                   'src="http://wx4.sinaimg.cn/wap180/7f840a09gy1ftf9244h1yj20yi0yi0wq.jpg" '
                   'alt="图片" class="ib" />'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'transmitNumber' at row 1")
2018-07-25 21:25:20 [scrapy.core.scraper] ERROR: Error processing {'blogger': 2139359753,
 'comeFrom': '07月22日 12:16&nbsp;来自超话',
 'commentNumber': '<img '
                  'src="http://wx4.sinaimg.cn/wap180/7f840a09gy1ftiinard81j20rs1dh7sh.jpg" '
                  'alt="图片" class="ib" />',
 'content': '',
 'goodNumber': '#双世宠妃#',
 'transmitNumber': '@邢昭林'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'commentNumber' at row 1")
2018-07-25 21:25:20 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:22 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:22 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5579672076,
 'fansTotal': '粉丝734人',
 'headshot': 'http://tvax3.sinaimg.cn/crop.1.0.1240.1240.50/005I5V7mly1fmvg5eevc1j30yi0ygdjx.jpg',
 'name': '赵鑫_7even旭'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:22 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5579672076,
 'fansTotal': '粉丝41人',
 'headshot': 'http://tvax1.sinaimg.cn/crop.0.0.248.248.50/007bmQwzly1fsrtjoqeazj306w06wq2y.jpg',
 'name': '倾心_beginslu'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:22 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5579672076,
 'fansTotal': '粉丝28人',
 'headshot': 'http://tvax1.sinaimg.cn/crop.0.0.259.259.50/0079WF9Mgy1fs168mb4lij3078078mx6.jpg',
 'name': '莪玩命伱随意_BILLNOTAR'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:22 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5579672076,
 'fansTotal': '粉丝37人',
 'headshot': 'http://tvax2.sinaimg.cn/crop.0.0.219.219.50/007aSsTsgy1fskxxov4exj3064064t8p.jpg',
 'name': '奈淬_vanilla__'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:22 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5579672076,
 'fansTotal': '粉丝32人',
 'headshot': 'http://tvax4.sinaimg.cn/crop.0.0.219.219.50/007adbjDgy1fs0tmzz871j30640640sq.jpg',
 'name': '毒心術_rairaku'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:22 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5579672076,
 'fansTotal': '粉丝28人',
 'headshot': 'http://tvax2.sinaimg.cn/crop.0.0.639.639.50/0079SgBOgy1fs8sd4trwsj30hs0hsgmw.jpg',
 'name': '兔枷壬_dannytry'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.cn/5579672076/fans> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
GeneratorExit
2018-07-25 21:25:22 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5579672076,
 'fansTotal': '粉丝30人',
 'headshot': 'http://tvax3.sinaimg.cn/crop.0.0.639.639.50/0079hy0Fgy1fs8tfwkuwlj30hs0hsgn0.jpg',
 'name': '邸胫共健_tiebob'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:22 [twisted] CRITICAL: Unhandled error in Deferred:
2018-07-25 21:25:22 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\task.py", line 517, in _oneWorkUnit
    result = next(self._iterator)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\defer.py", line 63, in <genexpr>
    work = (callable(elem, *args, **named) for elem in iterable)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\scraper.py", line 183, in _process_spidermw_output
    self.crawler.engine.crawl(request=output, spider=spider)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 210, in crawl
    self.schedule(request, spider)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 216, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 162, in enqueue_request
    if not request.dont_filter and self.df.request_seen(request):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\dupefilter.py", line 100, in request_seen
    added = self.server.sadd(self.key, fp)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1600, in sadd
    return self.execute_command('SADD', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:22 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:23 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:25 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:25 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:27 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:27 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5579672076,
 'comeFrom': '06月15日 06:30&nbsp;来自<a '
             'href="https://weibo.cn/sinaurl?f=w&amp;u=http%3A%2F%2Fdown.sina.cn%2Fsinaclient%2Fweibo%2Findex%2Fgsid%2F0%2Fmid%2F0">iPhone客户端</a>',
 'commentNumber': '评论[0]',
 'content': '路很直，但找不到方向 \u200b\u200b\u200b',
 'goodNumber': '赞[2]',
 'transmitNumber': '转发[0]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'comeFrom' at row 1")
2018-07-25 21:25:27 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5579672076,
 'comeFrom': '05月28日 14:09&nbsp;来自<a '
             'href="https://weibo.cn/sinaurl?f=w&amp;u=http%3A%2F%2Fdown.sina.cn%2Fsinaclient%2Fweibo%2Findex%2Fgsid%2F0%2Fmid%2F0">iPhone客户端</a>',
 'commentNumber': '原图',
 'content': '来过吧[允悲] ',
 'goodNumber': '显示地图',
 'transmitNumber': '<img '
                   'src="http://wx3.sinaimg.cn/wap180/0065BJAUly1frr0v7uiqyj33402c0b2b.jpg" '
                   'alt="图片" class="ib" />'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'comeFrom' at row 1")
2018-07-25 21:25:27 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5579672076,
 'comeFrom': '05月25日 13:24&nbsp;来自<a '
             'href="https://weibo.cn/sinaurl?f=w&amp;u=http%3A%2F%2Fdown.sina.cn%2Fsinaclient%2Fweibo%2Findex%2Fgsid%2F0%2Fmid%2F0">iPhone客户端</a>',
 'commentNumber': '评论[2]',
 'content': '睡个午觉，做梦做的一塌浮图 \u200b\u200b\u200b',
 'goodNumber': '赞[0]',
 'transmitNumber': '转发[0]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'comeFrom' at row 1")
2018-07-25 21:25:27 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5579672076,
 'comeFrom': '04月16日 21:01&nbsp;来自<a '
             'href="https://weibo.cn/sinaurl?f=w&amp;u=http%3A%2F%2Fdown.sina.cn%2Fsinaclient%2Fweibo%2Findex%2Fgsid%2F0%2Fmid%2F0">iPhone客户端</a>',
 'commentNumber': '评论[0]',
 'content': '操他妈的生活 \u200b\u200b\u200b',
 'goodNumber': '赞[2]',
 'transmitNumber': '转发[0]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'comeFrom' at row 1")
2018-07-25 21:25:27 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5579672076,
 'comeFrom': '05月18日 01:01&nbsp;来自生日动态',
 'commentNumber': '评论[0]',
 'content': '',
 'goodNumber': '赞[0]',
 'transmitNumber': '转发[0]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:27 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5579672076,
 'comeFrom': '03月06日 22:08&nbsp;来自<a '
             'href="https://weibo.cn/sinaurl?f=w&amp;u=http%3A%2F%2Fdown.sina.cn%2Fsinaclient%2Fweibo%2Findex%2Fgsid%2F0%2Fmid%2F0">iPhone客户端</a>',
 'commentNumber': '评论[1]',
 'content': '当一个男人放下面子去挣钱的时候证明他真的长大了 ',
 'goodNumber': '赞[1]',
 'transmitNumber': '转发[0]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'comeFrom' at row 1")
2018-07-25 21:25:27 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5579672076,
 'comeFrom': '02月22日 23:35&nbsp;来自<a '
             'href="https://weibo.cn/sinaurl?f=w&amp;u=http%3A%2F%2Fdown.sina.cn%2Fsinaclient%2Fweibo%2Findex%2Fgsid%2F0%2Fmid%2F0">iPhone客户端</a>',
 'commentNumber': '转发[0]',
 'content': '想想怎么突然就2018年了 ',
 'goodNumber': '显示地图',
 'transmitNumber': '赞[1]'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'comeFrom' at row 1")
2018-07-25 21:25:27 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5579672076,
 'comeFrom': '02月15日 22:36&nbsp;来自红包活动',
 'commentNumber': '赞[0]',
 'content': '',
 'goodNumber': '<img '
               'src="http://wx2.sinaimg.cn/wap180/67615c84ly1fnds9ik4vvj20kk0fcwgo.jpg" '
               'alt="图片" class="ib" />',
 'transmitNumber': '原图'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'goodNumber' at row 1")
2018-07-25 21:25:27 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5579672076,
 'comeFrom': '02月16日 00:05&nbsp;来自粉丝红包',
 'commentNumber': '赞[0]',
 'content': '',
 'goodNumber': '@吴昕',
 'transmitNumber': '吴昕 的红包'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 250, in inContext
    result = inContext.theWork()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\threadpool.py", line 266, in <lambda>
    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 122, in callWithContext
    return self.currentContext().callWithContext(ctx, func, *args, **kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\python\context.py", line 85, in callWithContext
    return func(*args,**kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\pipelines.py", line 66, in _process_item
    self.server.rpush(key, data)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 1388, in rpush
    return self.execute_command('RPUSH', name, *values)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-07-25 21:25:27 [scrapy.core.scraper] ERROR: Error processing {'blogger': 5579672076,
 'comeFrom': '02月15日 22:36&nbsp;来自红包活动',
 'commentNumber': '赞[0]',
 'content': '',
 'goodNumber': '<img '
               'src="http://wx2.sinaimg.cn/wap180/67615c84ly1fnds9ik4vvj20kk0fcwgo.jpg" '
               'alt="图片" class="ib" />',
 'transmitNumber': '原图'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\pipelines.py", line 33, in process_item
    db.otherOprate(sql, params=params)
  File "E:\pyfile\爬虫\scrapy+redis\sina_redis\sina\util\sqlUtil.py", line 51, in otherOprate
    self.cursor.execute(sql, params)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 165, in execute
    result = self._query(query)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\cursors.py", line 321, in _query
    conn.query(q)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 860, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1061, in _read_query_result
    result.read()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1349, in read
    first_packet = self.connection._read_packet()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 1018, in _read_packet
    packet.check_error()
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\connections.py", line 384, in check_error
    err.raise_mysql_exception(self._data)
  File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\pymysql\err.py", line 107, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1406, "Data too long for column 'goodNumber' at row 1")
2018-07-25 21:25:27 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:28 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:33 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:38 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:43 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:48 [scrapy.extensions.logstats] INFO: Crawled 16 pages (at 16 pages/min), scraped 0 items (at 0 items/min)
2018-07-25 21:25:48 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:53 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:25:58 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-07-25 21:26:03 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\crawler.py", line 291, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

